{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac4fc21d-fc01-43df-a6af-e16d4c55d0ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Clase 1 – Fundamentos y diagnóstico de rendimiento en Spark\n",
    "\n",
    "## Objetivos\n",
    "- Entender cómo ejecuta Spark un job\n",
    "- Diferenciar transformaciones y acciones\n",
    "- Analizar DAGs, stages y tasks\n",
    "- Identificar shuffles y problemas de rendimiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867a97b5-9327-45fa-9090-c3576f1b7eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 1 – Transformaciones vs Acciones\n",
    "\n",
    "Spark trabaja de forma **lazy**:  \n",
    "las transformaciones no se ejecutan hasta que aparece una acción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23a53171-216f-415c-a173-03eee3ee21d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformaciones\n",
    "- Operaciones que definen un nuevo DataFrame/RDD.\n",
    "- _Lazy evaluation_: no ejecutan nada hasta que llega una acción.\n",
    "- Son operaciones perezosas (_lazy evaluation_): no se ejecutan inmediatamente.\n",
    "- Devuelven un nuevo DataFrame/RDD con un plan de ejecución actualizado, pero sin lanzar un job.\n",
    "- Se ejecutan solo cuando se dispara una acción.\n",
    "- Pueden ser de dos tipos:\n",
    "    1. Narrow transformations: los datos de una partición se usan solo en esa misma partición.\n",
    "        - Ejemplos: `map()`, `filter()`, `select()`, `withColumn()`.\n",
    "        - Más eficientes (no requieren shuffle).\n",
    "    2. Wide transformations: requieren mover datos entre particiones → shuffle.\n",
    "        - Ejemplos: `groupBy()`, `join()`, `distinct()`, `repartition()`.\n",
    "        - Costosas en tiempo y recursos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a4e5148-ee23-4a7d-a58a-21d28cdeca74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Copia este fragmento en el Notebook\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.range(1, 100).withColumn(\"x2\", col(\"id\") * 2)  # transformación\n",
    "df_filtered = df.filter(col(\"id\") > 50)                   # transformación\n",
    "\n",
    "# Hasta aquí no se ejecuta nada\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "725002f8-34fd-478a-8c43-3c60000f84e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Acciones:\n",
    "- Disparan la ejecución real del plan.\n",
    "- Devuelven un resultado al driver o escriben datos en almacenamiento.\n",
    "- Ejemplos:\n",
    "  - `collect()` → trae todos los datos al driver (⚠️ cuidado con Out Of Memory).\n",
    "  - `show()` → muestra primeras filas en consola.\n",
    "  - `count()` → cuenta filas.\n",
    "  - `write.format(\"delta\").save(...)` → guarda en disco.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "332bb11f-c065-4810-a05c-f999322d7130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Copia este fragmento en el notebook y analiza\n",
    "```python\n",
    "df_filtered.show()   # aquí Spark ejecuta el pipeline\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "800e3b22-e7f6-4b39-8365-ea331dba3429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "301b6842-98a9-4827-88d5-c67719ed39fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Copia este fragmento en el notebook y analiza\n",
    "```python\n",
    "# Transformaciones (lazy)\n",
    "df = spark.range(1, 1000000)\n",
    "df2 = df.withColumn(\"x2\", col(\"id\") * 2)   # narrow\n",
    "df3 = df2.filter(col(\"x2\") % 5 == 0)       # narrow\n",
    "df4 = df3.groupBy((col(\"x2\") % 10)).count() # wide → shuffle\n",
    "\n",
    "# Acción (trigger)\n",
    "df4.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9855545b-42f9-4989-a6c4-5244d4abad3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Resultado:\n",
    "- Hasta `.groupBy()` solo hay transformaciones → Spark construye el DAG.\n",
    "- Al llamar `.show()`, Spark ejecuta el job, lo divide en stages y tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8578db13-9f70-4370-a4a3-b86f0b197c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resumen clave\n",
    "- Hasta que no hay una acción, Spark no ejecuta\n",
    "- Varias transformaciones se encadenan en un mismo job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ceb67cb-2af4-463c-b50c-292233db0b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 2 – DAG, Stages y Tasks\n",
    "Spark divide los pipelines en DAGs → stages → tasks.\n",
    "  \n",
    "Entender esto nos hace comprender cómo afecta al paralelismo y en definitiva al rendimiento.\n",
    "\n",
    "1. **DAG (Directed Acyclic Graph)**\n",
    "   - Qué es: Representación del flujo de transformaciones como un grafo dirigido sin ciclos.\n",
    "    - Cada nodo = operación (map, filter, join, etc.).\n",
    "    - Cada arista = dependencia de datos entre operaciones.\n",
    "    - Spark construye el DAG de manera perezosa (lazy evaluation).\n",
    "    - El DAG no se ejecuta hasta que se encuentra una acción (`.show()`, `.collect()`, `.count()`).\n",
    "\n",
    "2. **Stages**\n",
    "   - El DAG se divide en stages según los puntos de shuffle.\n",
    "   - Narrow dependency: cada partición depende solo de una partición anterior → mismo stage. Ejemplo: `map`, `filter`.\n",
    "   - Wide dependency: una partición depende de varias → nuevo stage. Ejemplo: `groupBy`, `join`, `distinct`.\n",
    "\n",
    "3. **Tasks**\n",
    "   - La unidad más pequeña de trabajo en Spark.\n",
    "   - Cada task procesa una partición de datos en un executor.\n",
    "   - Ejemplo: si tengo 200 particiones → Spark lanza 200 tasks (distribuidas en los executors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c97ead-de06-47c0-a6aa-a69fb278f397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo DAG Complejo\n",
    "Copia las siguientes celdas en el notebook y analiza lo que ocurre en cada caso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd700a0-6b97-4b4e-9846-858f41a70966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "# Ajustamos las particiones de shuffle para que se generen muchas tasks\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "\n",
    "# ==============================\n",
    "# DataFrame base (50 millones filas, 200 particiones iniciales)\n",
    "# ==============================\n",
    "df = spark.range(0, 50_000_000, numPartitions=200).withColumn(\"value\", (col(\"id\") * rand()))\n",
    "\n",
    "# ==============================\n",
    "# JOB 1: count con 2 stages\n",
    "# ==============================\n",
    "# Stage 1: lectura + filtro (narrow)\n",
    "df_filtered = df.filter(col(\"value\") > 1000)\n",
    "\n",
    "# Stage 2: shuffle por groupBy + count\n",
    "df_grouped = df_filtered.groupBy((col(\"id\") % 10).alias(\"bucket\")).count()\n",
    "\n",
    "# Acción que dispara Job 1\n",
    "print(\"Job 1 result:\", df_grouped.count())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7e5642-ade6-4ca8-8cf5-1a05d24d9e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "\n",
    "# Cuando llamamos a show(), Spark vuelve a ejecutar el pipeline porque es una acción.\n",
    "\n",
    "df_grouped.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7f808f3-df84-4afd-90db-be91f6d2dfe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "\n",
    "# Cuando llamamos a cache(), Spark vuelve a ejecutar el pipeline porque es una acción.\n",
    "# al igual que antes el job tiene 2 stages pero el segundo más largo con la instrucción de cache al final\n",
    "\n",
    "df_grouped.cache()\n",
    "```\n",
    "Aviso: Aquí al ejecutarlo va a dar un fallo porque la version gratis de databricks no permite cachear el df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35b6a5a-15d7-485e-ac9d-1d3ca989bc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "\n",
    "# Cuando llamamos a show(), Spark lee de la cache el df y eso ahorra tiempo\n",
    "\n",
    "df_grouped.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b3c81fc-c4a2-4d66-bd2e-69d2b89354f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```python\n",
    "\n",
    "# ==============================\n",
    "# JOB 2: join con 3 stages\n",
    "# ==============================\n",
    "df1 = spark.range(0, 10_000_000, numPartitions=100).withColumn(\"k\", col(\"id\") % 5000)\n",
    "df2 = spark.range(0, 20_000_000, numPartitions=150).withColumn(\"k\", col(\"id\") % 5000)\n",
    "\n",
    "# Stage 1: lectura df1\n",
    "# Stage 2: lectura df2\n",
    "# Stage 3: shuffle para join y acción final (show)\n",
    "df_joined = df1.join(df2, on=\"k\", how=\"inner\")\n",
    "\n",
    "# Acción que dispara Job 2\n",
    "df_joined.show(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f1efce1-2a78-4cbe-86ab-b5bdee52f783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejercicio DAGs\n",
    "Ve al Notebook y realiza los ejercicios de los DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c3cd0b-dc33-4b27-af64-d32c37b81877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Spark UI\n",
    "La Spark UI es la interfaz web que te muestra qué está pasando dentro de tu aplicación Spark. En Databricks puedes acceder desde la pestaña Spark UI de cada job o notebook.\n",
    "Los elementos principales son:\n",
    "1. Jobs tab\n",
    "   - Lista de trabajos disparados por acciones (count(), show(), write...).\n",
    "   - Cada job corresponde a la ejecución de un DAG.\n",
    "   - Desde aquí entras a Stages.\n",
    "2. Stages tab\n",
    "   - Muestra cómo se divide el job en stages (fases).\n",
    "   - Cada stage tiene múltiples tasks (una por partición).\n",
    "   - Métricas clave: tiempo, duración, skew, GC, I/O.\n",
    "3. Tasks tab (dentro de un stage)\n",
    "   - Detalle de cada partición ejecutada.\n",
    "   - Puedes detectar data skew: si una task tarda mucho más que las demás.\n",
    "   - Métricas de input size, shuffle read/write, memoria usada.\n",
    "4. SQL tab\n",
    "   - Muy útil si usas DataFrames/SQL.\n",
    "   - Visualiza el plan lógico y físico en forma de árbol.\n",
    "   - Identifica dónde ocurren shuffles y scans de tablas. \n",
    "5. Storage tab\n",
    "   - Muestra qué DataFrames/RDDs están cacheados.\n",
    "   - Permite ver el uso de memoria y disco para persistencia.\n",
    "\n",
    "En Databricks también tienes el DAG Viewer, un grafo visual que muestra stages y dependencias → muy útil para enseñar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a69cd5-9b7e-4a04-a9f6-43e3f5fcaa9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Interfaz general del Spark UI](https://docs.databricks.com/aws/en/assets/images/spark-ui-1effb898029c011d46288302118e96ce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf9b0802-d04c-41c0-87b7-ab9c3c294929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Spark UI para los stages](https://www.databricks.com/wp-content/uploads/2016/10/07-debug-spark-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b0e8ad-fb94-4196-9030-bb1e1ca7881e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://www.databricks.com/wp-content/uploads/2015/06/Screen-Shot-2015-06-19-at-2.00.59-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "133bbc96-fc07-4af9-a30d-14c830e2cc6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### `explained()`\n",
    "El método `.explain()` muestra cómo Spark planea ejecutar tu DataFrame.\n",
    "Opciones principales:\n",
    "- `.explain()` → plan físico resumido.\n",
    "- `.explain(\"extended\")` → plan lógico + optimizado + físico.\n",
    "- `.explain(\"cost\")` → incluye estimación de costes (filas, bytes).\n",
    "- `.explain(\"formatted\")` → salida legible en tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce950ee8-f1cb-4f7f-bd18-99e0e9ee7638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo práctica\n",
    "Copia este fragmento de código en el notebook y el resultado:\n",
    "```python\n",
    "# Crear DataFrame grande\n",
    "df = spark.range(0, 10000000)\n",
    "\n",
    "# Solo definimos transformaciones (lazy)\n",
    "df_filtered = df.filter(df.id % 2 == 0)\n",
    "df_transformed = df_filtered.withColumn(\"id_squared\", df.id * df.id)\n",
    "\n",
    "# Ver plan lógico/físico\n",
    "df_transformed.explain(\"formatted\")\n",
    "```\n",
    "\n",
    "Se debería ver algo parecido a esto:\n",
    "1. PhotonRange (1)\n",
    "   - Spark crea un dataset de enteros (de 0 a 10,000,000).\n",
    "   - Está dividido en 8 splits (= particiones iniciales).\n",
    "   - Paralelización inicial.\n",
    "2. PhotonFilter (2)\n",
    "   - Filtra solo los números pares: (id % 2 = 0).\n",
    "   - Es un narrow transformation (no hay shuffle).\n",
    "3. PhotonProject (3)\n",
    "   - Calcula una nueva columna id_squared.\n",
    "   - Otra transformación ligera, todavía sin shuffle.\n",
    "4. PhotonResultStage (4)\n",
    "   - Prepara los resultados para pasarlos al driver.\n",
    "5. ColumnarToRow (5)\n",
    "   - Convierte los datos del formato columnar interno (usado por Photon) a filas normales, porque el driver no entiende el formato columnar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad0f6b5-cec4-4a73-a65b-30ab77ef9a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejercicio - Uso `explained()`\n",
    "Ve al Notebook y realiza el ejercicio de esa sección"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdf2a739-8ac8-439a-82e1-893292ebaedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 3 – Shuffles, Data Skew y Particiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66f68a2-aff4-4b18-943b-b5802058c1f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Shuffles\n",
    "  - Redistribución de datos entre particiones.\n",
    "  - Se producen en `groupBy`, `join`, `distinct`, `orderBy`.\n",
    "  - Costosos porque implican:\n",
    "    - Escritura a disco (shuffle files).\n",
    "    - Transferencia por red.\n",
    "    - Lectura posterior.\n",
    "  - ⚠️ Los shuffles son la principal fuente de cuellos de botella en Spark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab654552-1b04-4e8f-aa18-2a6bacd64cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Skew (desequilibrio de datos) \n",
    "  - Cuando una clave concentra muchos más datos que el resto.\n",
    "  - Consecuencia: tareas desbalanceadas → unas rápidas, otras muy lentas.\n",
    "  - Efecto visible: long tail tasks.\n",
    "  - Ejemplo: 80% de las filas en la misma clave.\n",
    "\n",
    "  Imagina que tienes un df formado de esta forma:\n",
    "```python\n",
    "N = 10_000_000\n",
    "df = spark.range(0, N).withColumn(\n",
    "    \"skewed_key\",\n",
    "    when(col(\"id\") < int(N*0.99), lit(1))   # 95% de las filas van con la clave \"1\"\n",
    "    .otherwise((col(\"id\") % 1000) + 2)      # el 5% restante se reparte\n",
    ")\n",
    "\n",
    "df_grouped = df.groupBy(\"skewed_key\").count()\n",
    "```\n",
    "Va a ser un dataframe muy desbalanceado donde una sola clave (key = 0) concentra la mayoría de las filas y provocará que unas tareas terminan rápido y otras tardan muchísimo (long tail). \n",
    "\n",
    "Cuando se analicen sus tareas se verá así:\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*qI2TxDDpwZ4bzL3Vu_B3Hg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "878f151a-5ffe-42d6-a980-782b0dd7ec36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Particiones y paralelismo\n",
    "  - Spark divide el trabajo en particiones → tasks.\n",
    "  - Demasiadas particiones → overhead administrativo, demasiados archivos pequeños.\n",
    "  - Muy pocas particiones → subutilización de CPU, tareas gigantes que bloquean.\n",
    "  - Config clave:\n",
    "    - `spark.sql.shuffle.partitions` (por defecto: 200).\n",
    "    - `repartition()` y `coalesce()`.\n",
    "\n",
    "Para intentar corregir este problema se puede jugar con alguna de estas opciones:\n",
    "- `repartition(n)`\n",
    "- `coalesce(n)`\n",
    "- `spark.sql.shuffle.partitions`\n",
    "\n",
    "Prueba a ejecutar este fragmento de código para ver cómo varias las particiones cambiando el valor. 200 es el por defecto\n",
    "```python\n",
    "from pyspark.sql.functions import col, when, rand\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # valor inicial por defecto\n",
    "```\n",
    "\n",
    "  El rendimiento depende de encontrar el equilibrio justo."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "guia.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
