{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f0ac7af-3e2f-4792-b446-64157cc79ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingenier√≠a de Datos ‚Äì ETLs y Workflows con Delta Lake\n",
    "## Gu√≠a pr√°ctica (clase)\n",
    "\n",
    "Este notebook es la **gu√≠a te√≥rico-pr√°ctica** del bloque.\n",
    "Aqu√≠ **NO se programa en profundidad**, se explican conceptos y ejemplos.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo del bloque\n",
    "\n",
    "Entender c√≥mo pasar:\n",
    "- de **datos crudos**\n",
    "- a **pipelines ETL robustos**\n",
    "- listos para **producci√≥n** con Delta Lake y Workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b2ce5b1-9de5-42e6-b5f0-8da0ffbac49a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 1Ô∏è‚É£ Recordatorio ETL\n",
    "\n",
    "**ETL = Extract ‚Äì Transform ‚Äì Load**\n",
    "\n",
    "Un ETL (Extract, Transform, Load) es un proceso fundamental en la ingenier√≠a de datos. Consiste en extraer datos de diferentes fuentes, transformarlos para adecuarlos a las necesidades del negocio y cargarlos en un sistema de almacenamiento o an√°lisis, como un Data Lake o un Data Warehouse.\n",
    "\n",
    "### Extract\n",
    "En los entornos de Big Data, los datos pueden venir en m√∫ltiples formatos. Cada formato tiene ventajas y desventajas en cuanto a compresi√≥n, velocidad de lectura/escritura y compatibilidad. Parquet, por ejemplo, es columnar y eficiente para grandes vol√∫menes.\n",
    "- CSV\n",
    "- Parquet\n",
    "- JSON\n",
    "- Bases de datos\n",
    "- Streaming\n",
    "\n",
    "### Transform\n",
    "- Limpieza\n",
    "- Reglas de negocio\n",
    "- Normalizaci√≥n\n",
    "- Agregaciones\n",
    "\n",
    "### Load\n",
    "- Data Lake\n",
    "- Data Warehouse\n",
    "- Tablas Delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b695d88a-4c32-49c9-9310-6447310456b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde CSV\n",
    "Leer datos desde un archivo CSV es una de las formas m√°s comunes de ingesta en proyectos de datos. El formato CSV (Comma Separated Values) es ampliamente utilizado por su simplicidad y compatibilidad con la mayor√≠a de las herramientas. Pero no es el formato m√°s eficiente para grandes vol√∫menes de datos.\n",
    "\n",
    "<pre>\n",
    "df_csv = spark.read.csv(\n",
    "    path=path,\n",
    "    header=...,\n",
    "    inferSchema=...,\n",
    "    sep=...\n",
    ")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6ba3dfb-81fa-47ba-a6f1-2ceeecc6ea74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde parquet\n",
    "El formato Parquet es un est√°ndar de almacenamiento columnar ampliamente utilizado en Big Data. Su principal fortaleza es la eficiencia tanto en almacenamiento como en velocidad de lectura, especialmente cuando se trabaja con grandes vol√∫menes de datos y consultas sobre columnas espec√≠ficas. Parquet permite compresi√≥n y soporta tipos de datos complejos, lo que lo hace ideal para an√°lisis y procesamiento distribuido. \n",
    "\n",
    "<pre>\n",
    "df_parquet = spark.read.parquet(path)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0ffaa67-7626-46ac-9388-6718a9028bb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leer desde Json\n",
    "El formato JSON (JavaScript Object Notation) es ampliamente utilizado para el intercambio de datos debido a su flexibilidad y legibilidad. Permite almacenar estructuras de datos complejas, como listas y diccionarios anidados. Pero no es tan eficiente en almacenamiento ni en velocidad de procesamiento para grandes vol√∫menes de datos\n",
    "\n",
    "<pre>\n",
    "df_json = spark.read.json(path)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d910d7ed-3c01-4062-8afe-71305075f086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    " Fuente   | ‚úîÔ∏è Ventajas                                                                 | ‚ùå Desventajas                                                                | Uso Com√∫n                                               |\n",
    "----------|-------------------------------------------------------------------------|----------------------------------------------------------------------------|---------------------------------------------------------|\n",
    " CSV      | Simple y ampliamente compatible. F√°cil de editar manualmente.            | No soporta tipos de datos complejos ni compresi√≥n nativa. Menos eficiente para grandes vol√∫menes. | Ingesta inicial de datos peque√±os o de fuentes externas.|\n",
    " Parquet  | Almacenamiento columnar eficiente, compresi√≥n nativa, r√°pido para consultas en columnas espec√≠ficas. | No es legible por humanos. Requiere herramientas espec√≠ficas para edici√≥n.  | Procesamiento de Big Data, an√°lisis y almacenamiento optimizado.|\n",
    " JSON     | Flexible para estructuras complejas (anidadas). Legible y compatible con APIs web. | Menos eficiente en almacenamiento y procesamiento para grandes vol√∫menes.   | Intercambio de datos con sistemas web o cuando se necesita flexibilidad en la estructura.|\n",
    " BD       | Acceso directo a datos actualizados, integraci√≥n con sistemas empresariales, soporte para consultas SQL. | Requiere configuraci√≥n de conexi√≥n, puede tener limitaciones de rendimiento y permisos. | An√°lisis de datos operacionales, integraci√≥n de datos de negocio.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "432f1ac7-2594-4192-a076-c240fa32b365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Recordatorio: Leer con/sin esquema\n",
    "\n",
    "Al leer datos en Spark, se puede dejar que el sistema infiera autom√°ticamente el esquema (tipos de datos de cada columna) o definirlo expl√≠citamente. Inferir el esquema es c√≥modo y r√°pido para exploraciones iniciales, pero puede ser m√°s lento y propenso a errores si los datos son inconsistentes o si hay muchas columnas. Definir el esquema manualmente garantiza que los tipos de datos sean los esperados, mejora el rendimiento en la carga y ayuda a evitar problemas en etapas posteriores del procesamiento. Es una buena pr√°ctica definir el esquema en entornos productivos o cuando se requiere mayor control y robustez sobre los datos.\n",
    "\n",
    "Ejecuta el siguiente c√≥digo para leer sin esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8682e5d1-a22c-4259-9024-9d30390e8ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "| M√©todo de Lectura         | ‚úîÔ∏è Ventajas                                                                 | ‚ùå Desventajas                                                            | Uso Recomendado                          |\n",
    "|--------------------------|--------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------|\n",
    "| **Sin esquema (inferSchema=True)** | - R√°pido para exploraci√≥n inicial<br>- No requiere conocer los tipos de datos previamente | - Puede inferir tipos incorrectos si los datos son inconsistentes<br>- M√°s lento en archivos grandes<br>- Menos robusto en producci√≥n | Exploraci√≥n, pruebas r√°pidas, datos peque√±os o desconocidos |\n",
    "| **Con esquema definido** | - Tipos de datos consistentes y controlados<br>- Mejor rendimiento en la carga<br>- Evita errores por inferencia incorrecta | - Requiere conocer la estructura de los datos<br>- M√°s trabajo inicial | Procesos productivos, datos cr√≠ticos, ETLs, grandes vol√∫menes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6f020be-5daf-4380-98a8-d54b0647c183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<pre>\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Definir el esquema manualmente\n",
    "schema = StructType([\n",
    "    # Nombre, Tipo de dato, Requerido\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"fecha\", StringType(), True),\n",
    "    StructField(\"producto\", StringType(), True),\n",
    "    StructField(\"cantidad\", IntegerType(), True),\n",
    "    StructField(\"precio\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_csv = spark.read.csv(\n",
    "    path=base_path+\"ventas.csv\",\n",
    "    header=True,\n",
    "    schema=schema,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "df_csv.printSchema()\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "427c2e91-3b30-4f43-8eb2-868e99a44648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 2Ô∏è‚É£ Tipos de extracci√≥n\n",
    "\n",
    "### Carga total\n",
    "- Se borra y se vuelve a cargar todo\n",
    "- Simple\n",
    "- Poco eficiente\n",
    "\n",
    "### Carga incremental\n",
    "- Solo datos nuevos\n",
    "- Basada en fecha o ID\n",
    "- M√°s compleja, m√°s eficiente\n",
    "\n",
    "üëâ **En producci√≥n casi siempre incremental**\n",
    "\n",
    "Copia el siguiente bloque de c√≥digo en el notebook adaptando las rutas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a70ed5ac-2940-4dd9-898f-be72fa60ddf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<pre>\n",
    "# Ejemplo de carga total\n",
    "df_total = spark.read.csv(\"/Volumes/master/repaso/datos/total.csv\", header=True, inferSchema=True)\n",
    "display(df_total)\n",
    "\n",
    "# Ejemplo de carga incremental\n",
    "df_incremental = spark.read.csv(\"/Volumes/master/repaso/datos/incremental.csv\", header=True, inferSchema=True)\n",
    "df_incremental_load = df_incremental.join(df_total, on=df_total.columns, how=\"left_anti\")\n",
    "display(df_incremental_load)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fdb5e7e-45b5-48c2-bc37-83554514772d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 3Ô∏è‚É£ Transformaciones habituales\n",
    "\n",
    "### Limpieza\n",
    "- Eliminar nulos\n",
    "- Eliminar duplicados\n",
    "\n",
    "### Estandarizaci√≥n\n",
    "- Tipos de datos\n",
    "- Formatos de fecha\n",
    "\n",
    "### Reglas de negocio\n",
    "- Columnas calculadas\n",
    "- Flags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "726b2280-3254-4698-a377-66b0594e85c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ve copiando cada celda en el notebook y comprueba como va cambiando el df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5060269-dd01-4862-9a5b-7964370b6436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<pre>\n",
    "# Ejemplo de transformaciones habituales en Spark DataFrames\n",
    "\n",
    "from pyspark.sql.functions import col, trim, lower, when, regexp_replace\n",
    "\n",
    "# Creamos un DataFrame de ejemplo\n",
    "data = [\n",
    "    (1, \"  Juan  \", \"M\", \"Madrid\", 25, None),\n",
    "    (2, \"Ana\", \"F\", \"Barcelona\", 30, \"2026-01-07\"),\n",
    "    (3, \"Pedro\", None, \"Valencia\", None, \"2025-12-31\"),\n",
    "    (4, \"luc√≠a\", \"F\", \"Madrid\", 22, \"2026-01-01\"),\n",
    "    (5, \"Carlos\", \"M\", \"Sevilla\", 40, \"2026-01-05\"),\n",
    "    (6, \"  MAR√çA\", \"F\", \"Madrid\", 35, None)\n",
    "]\n",
    "columns = [\"id\", \"nombre\", \"genero\", \"ciudad\", \"edad\", \"fecha_registro\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e96b7ee4-afe6-4481-8592-2e4207019141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<pre>\n",
    "# 1. Limpieza: eliminar espacios y valores nulos\n",
    "df_limpio = df.withColumn(\"nombre\", trim(col(\"nombre\"))) \\\n",
    "              .na.fill({\"genero\": \"Desconocido\", \"edad\": 0})\n",
    "display(df_flags)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ecfca55-c2fa-4b33-8ba8-3d2b77926910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<pre>\n",
    "# 2. Estandarizaci√≥n: convertir a min√∫sculas y normalizar nombres de ciudad\n",
    "df_estandar = df_limpio.withColumn(\"nombre\", lower(col(\"nombre\"))) \\\n",
    "                       .withColumn(\"ciudad\", regexp_replace(lower(col(\"ciudad\")), \"madrid\", \"MADRID\"))\n",
    "display(df_flags)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e70a920-4867-4728-a160-b5bc651fd490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<pre>\n",
    "# 3. Reglas de negocio: crear flags\n",
    "df_flags = df_estandar.withColumn(\"es_madrid\", when(col(\"ciudad\") == \"MADRID\", 1).otherwise(0)) \\\n",
    "                      .withColumn(\"mayor_edad\", when(col(\"edad\") >= 18, 1).otherwise(0)) \\\n",
    "                      .withColumn(\"registro_reciente\", when(col(\"fecha_registro\") >= \"2026-01-01\", 1).otherwise(0))\n",
    "\n",
    "display(df_flags)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c34fe42-bd8e-431a-b862-194589012500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 4Ô∏è‚É£ ¬øPor qu√© Delta Lake?\n",
    "\n",
    "Delta Lake es una capa de almacenamiento open source que se integra con Apache Spark y a√±ade capacidades como:\n",
    "\n",
    "- Transacciones **ACID**:\n",
    "    - **Atomicity**: todo o nada\n",
    "    - **Consistency**: el esquema se respeta\n",
    "    - **Isolation**: escrituras concurrentes\n",
    "    - **Durability**: los datos persisten\n",
    "- **Schema enforcement**\n",
    "- **Time Travel**: Delta guarda un **log de transacciones**. Permite:\n",
    "    - Auditar cambios\n",
    "    - Recuperar datos\n",
    "    - Comparar versiones\n",
    "- Optimizaci√≥n de datos\n",
    "\n",
    "üëâ Sin Delta Lake, un Data Lake es solo almacenamiento\n",
    "\n",
    "##### ¬øPor qu√© es tan valioso el formato de Delta Table?\n",
    "Guardar tablas en formato Delta permite aprovechar las ventajas de transacciones ACID, manejo de versiones, y optimizaci√≥n de consultas. Es ideal para entornos donde los datos cambian frecuentemente y se requiere trazabilidad.\n",
    "\n",
    "Una Delta Table permite realizar operaciones ACID, mantener el hist√≥rico con time travel, gestionar versiones, optimizar el almacenamiento, y escalar en entornos de producci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af67fdd-210c-40a4-b50c-61211729b8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Guardar una tabla\n",
    "Guardar una tabla en Databricks puede hacerse de dos formas principales:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76be0985-d345-4a9b-b17f-bcabec70cef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "| M√©todo | Descripci√≥n | ‚úîÔ∏è Ventajas | Uso Recomendado |\n",
    "|--------|-------------|----------|-----------------|\n",
    "| **Por path** | Guarda los datos en una ruta espec√≠fica del sistema de archivos (ej: DBFS) usando formato Delta. | - Flexible y directo<br>- F√°cil de mover entre entornos<br>- Integraci√≥n con sistemas externos | Automatizaci√≥n, migraciones, acceso directo por ruta |\n",
    "| **Por cat√°logo del metastore** | Registra la tabla en el cat√°logo de Databricks para consultas SQL y control de acceso. | - Ideal para colaboraci√≥n multiusuario<br>- Control de permisos y versiones<br> - Auditor√≠a y seguridad avanzada<br>- F√°cil acceso mediante SQL | Entornos colaborativos, equipos de an√°lisis, integraci√≥n con BI |\n",
    "\n",
    "**Nota:** Ambas opciones aprovechan las ventajas del formato Delta: transacciones ACID, versionado y optimizaci√≥n de consultas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0753d916-bf69-42aa-9d0f-1dec15739a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Por path\n",
    "<pre>\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").save(...)\n",
    "</pre>\n",
    "\n",
    "Por catalogo\n",
    "<pre>\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"...\")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b137ebe0-72c8-4007-99da-3d27155716cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Leer una tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f863a0bc-236b-4622-8958-dd1a5f0c2efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "1. Lectura de una tabla Delta desde Spark: Tambi√©n puedes leer una tabla Delta directamente desde Spark usando el API de DataFrame:\n",
    "\n",
    "<pre>\n",
    "df_delta = spark.read.format(\"delta\").load(base_path+\"delta\")\n",
    "df_delta.show()\n",
    "</pre>\n",
    "\n",
    "Esto es √∫til cuando necesitas manipular los datos con Python, realizar transformaciones complejas, aplicar l√≥gica de negocio o integrarlo en pipelines de procesamiento.\n",
    "\n",
    "2. Consulta SQL sobre una tabla Delta: Puedes consultar una tabla Delta registrada en el cat√°logo usando SQL est√°ndar. Por ejemplo:\n",
    "\n",
    "<pre>\n",
    "SELECT * FROM ceste.productos;\n",
    "</pre>\n",
    "\n",
    "Esto te permite aprovechar toda la potencia del lenguaje SQL para filtrar, agrupar, unir y analizar los datos almacenados en formato Delta. Es especialmente √∫til para usuarios que prefieren trabajar con SQL o para integraciones con herramientas de BI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3962296b-c5d1-4806-9dfa-6a7ca075469c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "| M√©todo                |       ‚úîÔ∏è Ventajas        | Diferencias| Uso recomendado|\n",
    "|-----------------------|--------------------|------------|----------------|\n",
    "| **Spark DataFrame API** | - Procesamiento avanzado<br>- Integraci√≥n con ML y ETL<br>- Automatizaci√≥n y pipelines<br>- Flexibilidad en transformaciones                         | Permite l√≥gica compleja y manipulaci√≥n program√°tica de datos.  | Procesos autom√°ticos, machine learning, ETL, integraci√≥n con Python/Scala.|\n",
    "| **SQL**               | - F√°cil de usar y compartir<br>- Ideal para an√°lisis exploratorio<br>- Integraci√≥n con dashboards y BI<br>- Colaboraci√≥n multiusuario         | Sintaxis declarativa, acceso directo desde notebooks y herramientas BI.| An√°lisis, reporting, dashboards, colaboraci√≥n entre equipos.     |\n",
    "\n",
    "Ambos m√©todos aprovechan las ventajas de Delta Lake: transacciones ACID, versionado, rendimiento y escalabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398de85e-1abd-4360-bd0f-65c81216f3f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modificar una tabla\n",
    "Cuando trabajamos con tablas Delta, una de las grandes ventajas es la posibilidad de realizar operaciones transaccionales complejas de forma eficiente y segura como:\n",
    "- Updates\n",
    "- Deletes\n",
    "- Merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7e16bd0-c130-40f6-89eb-ceaaf6ea9118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<pre>\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, base_path+\"delta\")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498f04d2-0670-4433-9ee6-e660b1c93b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Update\n",
    "%md\n",
    "Permite modificar el valor de una o varias columnas en las filas que cumplen una condici√≥n espec√≠fica. Por ejemplo, se puede actualizar el nombre de un producto o corregir un valor err√≥neo en una tabla sin tener que reescribir todo el dataset. La sintaxis es similar a la de SQL, pero se realiza sobre la API de DeltaTable en Spark.\n",
    "\n",
    "<pre>\n",
    "# UPDATE\n",
    "delta_table.update(\n",
    "    condition=\"id = 1\",\n",
    "    set={\"producto\": \"'Ordenador'\"}\n",
    ")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6d0b0f9-50e7-4c6a-9110-f9344c21602f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Delete\n",
    "Permite eliminar filas que cumplen una condici√≥n determinada. Es √∫til para depurar datos, eliminar registros obsoletos o cumplir con requisitos legales de borrado. Al igual que el update, el delete es transaccional y garantiza la integridad de la tabla.\n",
    "\n",
    "<pre>\n",
    "delta_table.delete(\"precio > 150\")\n",
    "</pre>\n",
    "%md\n",
    "Ambas operaciones aprovechan las transacciones ACID de Delta Lake, lo que significa que los cambios son at√≥micos, consistentes, aislados y duraderos. Esto evita problemas de concurrencia y asegura que los datos siempre est√©n en un estado v√°lido, incluso en entornos multiusuario o de procesamiento distribuido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a10e3e35-e4b9-40e6-8815-d24d77b83bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Merge\n",
    "En este bloque de c√≥digo se muestra c√≥mo realizar un \"merge\" (tambi√©n conocido como upsert) sobre una tabla Delta:\n",
    "\n",
    "1. Se crea un DataFrame con nuevos datos o datos actualizados.\n",
    "2. Luego, se utiliza el m√©todo `merge` de la API de Delta Lake para comparar los datos existentes en la tabla (target) con los nuevos datos (source) usando una condici√≥n de emparejamiento (en este caso, el campo id).\n",
    "3. Si el `id` ya existe en la tabla, se actualizan todos los campos de ese registro (`whenMatchedUpdateAll`).\n",
    "4. Si el `id` no existe, se inserta el nuevo registro (`whenNotMatchedInsertAll`).\n",
    "\n",
    "<pre>\n",
    "# Nuevos datos a insertar/actualizar\n",
    "columns = [\"id\", \"fecha\", \"producto\", \"cantidad\", \"precio\"]\n",
    "\n",
    "nuevos_datos = [(3, \"2025-05-24\", \"Monitor\", 1, 179.99), (4, \"2025-05-24\", \"Impresora\", 2, 89.99)]\n",
    "df_updates = spark.createDataFrame(nuevos_datos, columns)\n",
    "\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_updates.alias(\"source\"),\n",
    "    \"target.id = source.id\") \\\n",
    "  .whenMatchedUpdateAll() \\\n",
    "  .whenNotMatchedInsertAll() \\\n",
    "  .execute()\n",
    "  </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0023b18-806e-4725-adc9-4ea1df854f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Este tipo de operaci√≥n es fundamental en escenarios de integraci√≥n incremental de datos, donde peri√≥dicamente llegan nuevos registros o actualizaciones y queremos mantener la tabla Delta siempre actualizada y sin duplicados.\n",
    "\n",
    "Ventajas de usar merge en Delta Lake:\n",
    "\n",
    "- Permite mantener la integridad y consistencia de los datos.\n",
    "- Facilita la implementaci√≥n de pipelines de datos incrementales.\n",
    "- Aprovecha las transacciones ACID de Delta Lake, evitando problemas de concurrencia o corrupci√≥n de datos.\n",
    "- Es mucho m√°s eficiente y sencillo que realizar operaciones manuales de actualizaci√≥n e inserci√≥n por separado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff19e3f-eeee-4f8a-9bcb-830377d22473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time Travel\n",
    "\n",
    "El Time Travel en Delta Lake es una funcionalidad que permite consultar versiones anteriores de una tabla Delta. Cada vez que se realiza una operaci√≥n de escritura (insert, update, delete, merge), Delta Lake crea una nueva versi√≥n de la tabla, manteniendo el historial de cambios.\n",
    "\n",
    "**¬øPara qu√© sirve el Time Travel?**\n",
    "- Recuperar datos borrados o modificados accidentalmente.\n",
    "- Auditar cambios y analizar c√≥mo han evolucionado los datos a lo largo del tiempo.\n",
    "- Comparar el estado de la tabla en diferentes momentos.\n",
    "- Reproducir experimentos o an√°lisis sobre datos hist√≥ricos.\n",
    "\n",
    "**¬øC√≥mo se usa?**\n",
    "Puedes acceder a una versi√≥n anterior de la tabla especificando el n√∫mero de versi√≥n (`versionAsOf`) o una marca de tiempo (`timestampAsOf`) al leer los datos:\n",
    "\n",
    "**Ventajas:**\n",
    "- No necesitas mantener copias manuales de los datos para auditor√≠a o recuperaci√≥n.\n",
    "- Todas las operaciones de Time Travel son transaccionales y consistentes.\n",
    "- Facilita la trazabilidad y el cumplimiento normativo en entornos empresariales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "747c1d11-ae0c-4984-a8d9-dcd863bd8ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Copia las siguientes celdas en el Notebook y mira lo que pasa\n",
    "\n",
    "Ver historial\n",
    "<pre>\n",
    "display(delta_table.history())\n",
    "</pre>\n",
    "\n",
    "Leer versi√≥n anterior\n",
    "<pre>\n",
    "df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(base_path+\"delta\")\n",
    "display(df_old)\n",
    "</pre>\n",
    "\n",
    "Leer la tabla tal como estaba en una fecha concreta\n",
    "<pre>\n",
    "df_moment = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2025-05-27 10:00:00\").load(base_path+\"delta\")\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f94b0a-ffa6-49d7-861c-8f610b776ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimizaci√≥n de Tablas\n",
    "%md\n",
    "La optimizaci√≥n de tablas en Delta Lake es clave para mejorar el rendimiento de las consultas y reducir el coste de almacenamiento en entornos de Big Data. Existen dos t√©cnicas principales:\n",
    "\n",
    "- **Compactaci√≥n (OPTIMIZE):** Consiste en reducir el n√∫mero de archivos peque√±os que se generan tras m√∫ltiples escrituras o actualizaciones. Al compactar, se agrupan estos archivos en otros m√°s grandes, lo que acelera las lecturas y reduce la sobrecarga de gesti√≥n de archivos en el sistema distribuido.\n",
    "\n",
    "- **Z-Ordering:** Es una t√©cnica de ordenaci√≥n f√≠sica de los datos en disco basada en una o varias columnas clave. Al aplicar Z-Ordering, los datos se almacenan de forma que las filas con valores similares en las columnas seleccionadas queden f√≠sicamente pr√≥ximas. Esto mejora notablemente el rendimiento de las consultas filtradas por esas columnas, ya que minimiza la cantidad de datos que Spark necesita leer.\n",
    "\n",
    "**Ventajas de la optimizaci√≥n:**\n",
    "- Consultas m√°s r√°pidas y eficientes, especialmente en grandes vol√∫menes de datos.\n",
    "- Menor latencia en dashboards y an√°lisis interactivos.\n",
    "- Reducci√≥n de costes de almacenamiento y procesamiento.\n",
    "- Mejor aprovechamiento de los recursos del cluster.\n",
    "\n",
    "**Cu√°ndo optimizar:**\n",
    "- Tras cargas masivas de datos o procesos ETL frecuentes.\n",
    "- Cuando se detecta degradaci√≥n en el rendimiento de las consultas.\n",
    "- Antes de ejecutar an√°lisis cr√≠ticos o dashboards de negocio.\n",
    "\n",
    "En resumen, la optimizaci√≥n peri√≥dica de las tablas Delta es una buena pr√°ctica para mantener el entorno √°gil, eficiente y escalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c988cb0d-ece7-4d14-aa39-638e147fa1d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Optimizar tabla\n",
    "<pre>\n",
    "spark.sql(\"OPTIMIZE ceste.productos\")\n",
    "</pre>\n",
    "\n",
    "Ordenar f√≠sicamente por \"id\"\n",
    "<pre>\n",
    "spark.sql(\"OPTIMIZE ceste.productos ZORDER BY id\")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a20fde0f-a3a2-4ca9-8b88-ef4b3d40daab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 5Ô∏è‚É£ Transacciones ACID (muy importante)\n",
    "\n",
    "- **Atomicity**: todo o nada\n",
    "- **Consistency**: el esquema se respeta\n",
    "- **Isolation**: escrituras concurrentes\n",
    "- **Durability**: los datos persisten\n",
    "\n",
    "üëâ Clave para entornos productivos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20115250-533e-46ff-97e8-c9d1a4b6f674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 6Ô∏è‚É£ Time Travel\n",
    "\n",
    "Delta guarda un **log de transacciones**.\n",
    "\n",
    "Permite:\n",
    "- Auditar cambios\n",
    "- Recuperar datos\n",
    "- Comparar versiones\n",
    "\n",
    "üëâ Muy usado para debugging y errores humanos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "230338fa-d77e-45ef-b81a-5807da7bc3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631dc0e6-707d-4f8c-ac72-cd0d2d5ee5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 7Ô∏è‚É£ Modelado y pipelines\n",
    "\n",
    "Arquitectura t√≠pica:\n",
    "\n",
    "### Bronze\n",
    "- Datos crudos\n",
    "\n",
    "### Silver\n",
    "- Datos limpios y transformados\n",
    "\n",
    "### Gold\n",
    "- Datos agregados y listos para negocio\n",
    "\n",
    "üëâ Cada capa suele ser un notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a873607-bf5a-4da8-9be8-666c63fed8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 8Ô∏è‚É£ Jobs y Workflows\n",
    "\n",
    "### Job\n",
    "- Ejecuta un notebook\n",
    "- Programado o manual\n",
    "\n",
    "### Workflow\n",
    "- Encadena m√∫ltiples Jobs\n",
    "- Controla dependencias\n",
    "- Maneja errores\n",
    "\n",
    "üëâ Es lo que se usa en producci√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afb7393c-6e03-4a4b-9a03-f2f4ebc10d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## ‚úÖ Mensaje final\n",
    "\n",
    "Un buen pipeline:\n",
    "- Es **autom√°tico**\n",
    "- Es **reproducible**\n",
    "- Falla de forma controlada\n",
    "- Est√° preparado para crecer\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "guia",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
